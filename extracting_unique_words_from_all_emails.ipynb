{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7G_HLWad6KOC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import time\n",
    "import string\n",
    "import operator\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sarath\n",
      "[nltk_data]     chandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sarath\n",
      "[nltk_data]     chandra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IvZwqraX6Pv9"
   },
   "outputs": [],
   "source": [
    "def text_cleanup(text):\n",
    "    text_without_punctuation = [c for c in text if c not in string.punctuation]\n",
    "    text_without_punctuation = ''.join(text_without_punctuation)\n",
    "    text_without_stopwords = [word for word in text_without_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "    text_without_stopwords = ' '.join(text_without_stopwords)\n",
    "    cleaned_text = [word.lower() for word in text_without_stopwords.split()]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DSXwdQl46huK"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EgGCVLma6y-R"
   },
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "k=0\n",
    "count = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VKsPXwH98PO3"
   },
   "outputs": [],
   "source": [
    "directory_in_str = \"emails/\"\n",
    "directory = os.fsencode(directory_in_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 100\n",
      "Done 200\n",
      "Done 300\n",
      "Done 400\n",
      "Done 500\n",
      "Done 600\n",
      "Done 700\n",
      "Done 800\n",
      "Done 900\n",
      "Done 1000\n",
      "Done 1100\n",
      "Done 1200\n",
      "Done 1300\n",
      "Done 1400\n",
      "Done 1500\n",
      "Done 1600\n",
      "Done 1700\n",
      "Done 1800\n",
      "Done 1900\n",
      "Done 2000\n",
      "Done 2100\n",
      "Done 2200\n",
      "Done 2300\n",
      "Done 2400\n",
      "Done 2500\n",
      "Done 2600\n",
      "Done 2700\n",
      "Done 2800\n",
      "Done 2900\n",
      "Done 3000\n",
      "Done 3100\n",
      "Done 3200\n",
      "Done 3300\n",
      "Done 3400\n",
      "Done 3500\n",
      "Done 3600\n",
      "Done 3700\n",
      "Done 3800\n",
      "Done 3900\n",
      "Done 4000\n",
      "Done 4100\n",
      "Done 4200\n",
      "Done 4300\n",
      "Done 4400\n",
      "Done 4500\n",
      "Done 4600\n",
      "Done 4700\n",
      "Done 4800\n",
      "Done 4900\n",
      "Done 5000\n",
      "Done 5100\n",
      "Done 5200\n",
      "Done 5300\n",
      "Done 5400\n",
      "Done 5500\n",
      "Done 5600\n",
      "Done 5700\n",
      "Done 5800\n",
      "Done 5900\n",
      "Done 6000\n",
      "Done 6100\n",
      "Done 6200\n",
      "Done 6300\n",
      "Done 6400\n",
      "Done 6500\n",
      "Done 6600\n",
      "Done 6700\n",
      "Done 6800\n",
      "Done 6900\n",
      "Done 7000\n",
      "Done 7100\n",
      "Done 7200\n",
      "Done 7300\n",
      "Done 7400\n",
      "Done 7500\n",
      "Done 7600\n",
      "Done 7700\n",
      "Done 7800\n",
      "Done 7900\n",
      "Done 8000\n",
      "Done 8100\n",
      "Done 8200\n",
      "Done 8300\n",
      "Done 8400\n",
      "Done 8500\n",
      "Done 8600\n",
      "Done 8700\n",
      "Done 8800\n",
      "Done 8900\n",
      "Done 9000\n",
      "Done 9100\n",
      "Done 9200\n",
      "Done 9300\n",
      "Done 9400\n",
      "Done 9500\n",
      "Done 9600\n",
      "Done 9700\n",
      "Done 9800\n",
      "Done 9900\n",
      "Done 10000\n",
      "Done 10100\n",
      "Done 10200\n",
      "Done 10300\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(directory):\n",
    "    file = file.decode(\"utf-8\")\n",
    "    file_name = str(os.getcwd()) + '/emails/'\n",
    "    file_name = file_name + file\n",
    "    file_reading = open(file_name,\"r\",encoding='utf-8', errors='ignore')\n",
    "    words = text_cleanup(file_reading.read())\n",
    "    for word in words:\n",
    "        if (word.isdigit()==False and len(word)>2):\n",
    "            word = lmtzr.lemmatize(word)\n",
    "            if word in count:\n",
    "                count[word] += 1\n",
    "            else:\n",
    "                count[word] = 1\n",
    "    k+=1\n",
    "    if(k%100==0):\n",
    "        print(\"Done \" + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1bJlUAex8WiS"
   },
   "outputs": [],
   "source": [
    "sorted_count = sorted(count.items(),key=operator.itemgetter(1),reverse=True)\n",
    "sorted_count = dict(sorted_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cJFvogor8hWp"
   },
   "outputs": [],
   "source": [
    "f= open(\"wordslist.csv\",\"w+\")\n",
    "f.write('word,count')\n",
    "f.write('\\n')\n",
    "for word , times in sorted_count.items():\n",
    "    if times < 100:\n",
    "        break\n",
    "    f.write(str(word) + ',' + str(times))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sW46FjxV8nDD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (in seconds) to pre process the emails 294.92\n"
     ]
    }
   ],
   "source": [
    "print('Time (in seconds) to pre process the emails ' + str(round(time.time() - start_time,2)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
